{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"../Data/labeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv(\"../Data/testData.tsv\", header=0, delimiter='\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"../Data/unlabeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# put all above works into a function\n",
    "def review_to_words(raw_review, stop_words=False):\n",
    "    '''\n",
    "    This is a function to convert a raw review to a string of words\n",
    "    Input is a raw moive review \n",
    "    Output is a preprocessed movie review\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove HTML by BeautifulSoup\n",
    "    2. Remove non-letter by Reg Exp\n",
    "    3. Convert to lower case and split\n",
    "    4. Remove stop words (Optional)\n",
    "    5. Join the word back and return\n",
    "    '''\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    word_list = letters_only.lower().split()\n",
    "    if stop_words:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_list = [word for word in word_list if word not in stop_words]\n",
    "    return word_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, stop_words=False):\n",
    "    # split review string into a list of sentences\n",
    "    review = review.decode(\"utf8\")\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    # split each sentence into a list of words\n",
    "    sentences_list = []\n",
    "    for sentence in raw_sentences:\n",
    "        if len(sentence) > 0:\n",
    "            sentences_list.append(review_to_words(sentence, stop_words))\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "# parsing sentences from training set\n",
    "for review in train[\"review\"]:\n",
    "    # sentences is a list of list since we use + not append here\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "# parsing sentences from unlabeled set\n",
    "for review in unlabeled_train['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again']\n",
      "[u'maybe', u'i', u'just', u'want', u'to', u'get', u'a', u'certain', u'insight', u'into', u'this', u'guy', u'who', u'i', u'thought', u'was', u'really', u'cool', u'in', u'the', u'eighties', u'just', u'to', u'maybe', u'make', u'up', u'my', u'mind', u'whether', u'he', u'is', u'guilty', u'or', u'innocent']\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)\n",
    "print sentences[0]\n",
    "print sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers=num_features, size=num_features, \n",
    "                         min_count=min_word_count, window=context, sample=downsampling)\n",
    "\n",
    "# if you dont want to train the model any further, calling init_sims will \n",
    "# make the model more memory-efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save the model for later use. \n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.624958872795105),\n",
       " (u'lad', 0.6053160429000854),\n",
       " (u'lady', 0.5940924882888794),\n",
       " (u'chap', 0.5568241477012634),\n",
       " (u'monk', 0.5337693691253662),\n",
       " (u'men', 0.5267441272735596),\n",
       " (u'guy', 0.5256942510604858),\n",
       " (u'millionaire', 0.5067365169525146),\n",
       " (u'person', 0.5048238635063171),\n",
       " (u'priest', 0.502406895160675)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'princess', 0.6811302304267883),\n",
       " (u'bride', 0.6305873394012451),\n",
       " (u'latifah', 0.6072506904602051),\n",
       " (u'stepmother', 0.590834379196167),\n",
       " (u'mistress', 0.5827232003211975),\n",
       " (u'countess', 0.5789092779159546),\n",
       " (u'maid', 0.5775595903396606),\n",
       " (u'showgirl', 0.5737118721008301),\n",
       " (u'belle', 0.5697005987167358),\n",
       " (u'elizabeth', 0.5634185075759888)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.7785122394561768),\n",
       " (u'atrocious', 0.74686598777771),\n",
       " (u'horrible', 0.7442623972892761),\n",
       " (u'abysmal', 0.718186616897583),\n",
       " (u'dreadful', 0.7085167169570923),\n",
       " (u'horrendous', 0.6890318989753723),\n",
       " (u'horrid', 0.6825686097145081),\n",
       " (u'appalling', 0.6796859502792358),\n",
       " (u'lousy', 0.6333217620849609),\n",
       " (u'amateurish', 0.6245442628860474)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
